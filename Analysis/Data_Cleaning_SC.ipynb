{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_data_as_dataframe(data_path, Animal_ID, protocol, data_type, unique_value_threshold=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): path to the data folder\n",
    "        Animal_ID (str): animal ID\n",
    "        protocol (str): protocol name\n",
    "        data_type (str): data type (e.g. 'Trial_Summary', 'Detected_Licks')\n",
    "        unique_value_threshold (int): maximum number of unique values to retrieve per column\n",
    "\n",
    "    Returns:\n",
    "        multiindex_df (pd.DataFrame): MultiIndex DataFrame containing information for each CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    subject_folders = glob.glob(f\"{data_path}/{Animal_ID}/*\")\n",
    "    protocol_folders = [folder for folder in subject_folders if f\"{protocol}_{Animal_ID}\" in folder]\n",
    "\n",
    "    if not protocol_folders:\n",
    "        warnings.warn(f\"{Animal_ID}: protocol folders empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = []\n",
    "    for folder in tqdm(protocol_folders, position=0, leave=True, desc=f'Processing {Animal_ID}'):\n",
    "        folder_files = glob.glob(f\"{folder}/**/{data_type}*.csv\", recursive=True)\n",
    "        for file_path in folder_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Check if the DataFrame is empty (only headers, no data rows)\n",
    "                if df.empty:\n",
    "                    # If the DataFrame is empty, skip this file and continue with the next one\n",
    "                    continue\n",
    "                # Shorten the file path\n",
    "                short_path = '/'.join(file_path.split('/')[-3:])\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                creation_timestamp = os.path.getctime(file_path)\n",
    "                creation_time_human = datetime.datetime.fromtimestamp(creation_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                num_rows = len(df)\n",
    "                num_columns = len(df.columns)  # Number of columns in the dataframe\n",
    "\n",
    "                for column in df.columns:\n",
    "                    unique_types = df[column].dropna().apply(lambda x: type(x).__name__).unique()[:unique_value_threshold]\n",
    "                    unique_values = df[column].dropna().unique()[:unique_value_threshold]\n",
    "                    num_missing = df[column].isnull().sum()\n",
    "                        # When Info_Type is unique_types and the Data column is empty, fill it with NaN\n",
    "                    # if 'unique_types' in df['Info_Type'].values and df[column].isnull().all():\n",
    "                    #     df.loc[df['Info_Type'] == 'unique_types', 'Data'] = np.nan\n",
    "                    # Calculate summary statistics only for numerical columns\n",
    "                    if df[column].dtype in [np.int64, np.float64]:\n",
    "                        summary_stats = df[column].describe().to_dict()\n",
    "                        column_data = {\n",
    "                            'unique_types': ', '.join(unique_types),\n",
    "                            'unique_values': ', '.join(map(str, unique_values)),\n",
    "                            'num_missing': num_missing,\n",
    "                            **summary_stats\n",
    "                        }\n",
    "                    else:\n",
    "                        column_data = {\n",
    "                            'unique_types': ', '.join(unique_types),\n",
    "                            'unique_values': ', '.join(map(str, unique_values)),\n",
    "                            'num_missing': num_missing\n",
    "                        }\n",
    "                    for info_type, value in column_data.items():\n",
    "                        data.append((short_path, column, info_type, value))\n",
    "\n",
    "                # Add file metadata including creation timestamp and number of columns\n",
    "                data.append((short_path, 'File_Metadata', 'file_size', file_size))\n",
    "                data.append((short_path, 'File_Metadata', 'num_rows', num_rows))\n",
    "                data.append((short_path, 'File_Metadata', 'num_columns', num_columns))  # Adding num_columns\n",
    "                data.append((short_path, 'File_Metadata', 'creation_timestamp', creation_time_human))\n",
    "\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Creating MultiIndex DataFrame\n",
    "    multiindex_df = pd.DataFrame(data, columns=['File_Path', 'Column', 'Info_Type', 'Data'])\n",
    "    multiindex_df.set_index(['File_Path', 'Column', 'Info_Type'], inplace=True)\n",
    "\n",
    "    return multiindex_df\n",
    "\n",
    "def get_animal_data(data_path, Animal_ID, protocol, data_type):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_path (str): path to the data folder\n",
    "        Animal_ID (str): animal ID\n",
    "        protocol (str): protocol name\n",
    "        data_type (str): data type (e.g. 'Trial_Summary', 'Detected_Licks')\n",
    "    Returns:\n",
    "        animal_df (pd.DataFrame): dataframe containing all the data for the animal\n",
    "    \"\"\"\n",
    "\n",
    "    subject_folders = glob.glob(data_path + Animal_ID +'/*') \n",
    "    protocol_folders = list(filter(lambda x: protocol in x, subject_folders)) \n",
    "\n",
    "    if len(protocol_folders) == 0:\n",
    "        warnings.warn(f\"{Animal_ID}: all_folders empty\")\n",
    "        return 0\n",
    "\n",
    "    sessions_data = []\n",
    "    for folder in tqdm(protocol_folders, position=0, leave=True, desc = 'Processing ' + Animal_ID ):  \n",
    "        folder_files = glob.glob(folder +'/**/'+data_type +'*.csv', recursive = True)\n",
    "        for file in folder_files:\n",
    "            try:\n",
    "                session_df = pd.read_csv(file)\n",
    "\n",
    "                # # Explicitly cast columns with all-bool values to boolean type\n",
    "                # for col in session_df.columns:\n",
    "                #     if session_df[col].dtype == 'object' and set(session_df[col].unique()) <= {True, False, None}:\n",
    "                #         session_df[col] = session_df[col].astype('bool')\n",
    "\n",
    "                date_pattern = r'\\d{4}_\\d{1,2}_\\d{1,2}'\n",
    "                date = re.search(date_pattern, file).group(0)\n",
    "                date_obj = datetime.datetime.strptime(date, '%Y_%m_%d')\n",
    "                formatted_date = date_obj.strftime('%Y/%m/%d')\n",
    "                session_df.insert(0, 'Date', formatted_date)\n",
    "                session_df['File_ID'] = file  # Add file identifier\n",
    "                sessions_data.append(session_df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                pass\n",
    "\n",
    "        animal_df = pd.concat(sessions_data, axis=0, ignore_index=True)\n",
    "        # Create a list of columns for sorting\n",
    "        sort_columns = ['Date']\n",
    "\n",
    "        # Check which of the columns ('Trial_End_Time' or 'Time') exists in the DataFrame\n",
    "        if 'Trial_End_Time' in animal_df.columns:\n",
    "            sort_columns.append('Trial_End_Time')\n",
    "        elif 'Time' in animal_df.columns:\n",
    "            sort_columns.append('Time')\n",
    "\n",
    "        # Sort the DataFrame by the determined columns\n",
    "        animal_df = animal_df.sort_values(by=sort_columns)\n",
    "\n",
    "        date_list = []\n",
    "        for date in animal_df.Date.unique():\n",
    "            date_df = animal_df[animal_df.Date == date].reset_index(drop=True)\n",
    "            date_list.append(date_df)\n",
    "            \n",
    "        animal_df = pd.concat(date_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return animal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CSV Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QP093:   1%|          | 1/95 [00:00<00:09,  9.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QP093: 100%|██████████| 95/95 [00:12<00:00,  7.61it/s]\n",
      "Processing QP094: 100%|██████████| 99/99 [00:16<00:00,  6.05it/s]\n",
      "Processing QP096: 100%|██████████| 41/41 [00:10<00:00,  3.80it/s]\n",
      "Processing QP097: 100%|██████████| 97/97 [00:12<00:00,  7.56it/s]\n",
      "Processing QP098: 100%|██████████| 99/99 [00:14<00:00,  7.03it/s]\n",
      "Processing QP099: 100%|██████████| 97/97 [00:13<00:00,  7.39it/s]\n",
      "Processing QP0100: 100%|██████████| 95/95 [00:11<00:00,  8.38it/s]\n",
      "Processing QP0101: 100%|██████████| 94/94 [00:16<00:00,  5.76it/s]\n",
      "/var/folders/36/99q57ldx4j956537wjswtd4c0000gn/T/ipykernel_21620/109678336.py:18: UserWarning: QP0102: protocol folders empty\n",
      "  warnings.warn(f\"{Animal_ID}: protocol folders empty\")\n",
      "Processing QP0103: 100%|██████████| 78/78 [00:42<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Volumes/akrami/Quentin/Head_Fixed_Behavior/Data/'\n",
    "protocol = 'SOUND_CAT'\n",
    "data_type = 'Trial_Summary'\n",
    "\n",
    "Animal_List = ['QP093', 'QP094', 'QP096', 'QP097', 'QP098', 'QP099', 'QP0100', 'QP0101', 'QP0102', 'QP0103']\n",
    "\n",
    "\n",
    "# Define the path to the cache file\n",
    "cache_path = '/Users/quentin/Desktop/HeadFixedBehavior/Data/'\n",
    "cache_file = os.path.join(cache_path, \"all_csv_info_SC.pkl\")\n",
    "\n",
    "# Define the path to the figures\n",
    "figs_path = '/Users/quentin/Desktop/HeadFixedBehavior/Analysis/Figures/'\n",
    "\n",
    "# Check if the cache file exists\n",
    "if os.path.exists(cache_file):\n",
    "    # If the cache file exists, load the data from the cache\n",
    "    with open(cache_file, \"rb\") as f:       \n",
    "        csv_info = pickle.load(f)\n",
    "else:\n",
    "    # If the cache file does not exist, load the data from the remote server\n",
    "    csv_info = pd.DataFrame()\n",
    "    for animal in Animal_List:\n",
    "        try:\n",
    "            # Load the data for the current animal and add it to the dictionary\n",
    "            animal_data = get_csv_data_as_dataframe(data_path,\n",
    "                                             animal, \n",
    "                                             protocol,\n",
    "                                             data_type)\n",
    "            csv_info = pd.concat([csv_info, animal_data])\n",
    "        except:\n",
    "            # If there is an error loading the data, skip the current animal\n",
    "            pass\n",
    "\n",
    "    # Save the data to the cache file\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(csv_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_info_reset = csv_info.reset_index()\n",
    "# whenever there is a NaN in the Data column and the Info_Type is unique_types, fill it with NaN\n",
    "csv_info_reset.loc[csv_info_reset['Info_Type'] == 'unique_types', 'Data'] = csv_info_reset.loc[csv_info_reset['Info_Type'] == 'unique_types', 'Data'].replace('', np.nan)\n",
    "# sace the csv_info_reset to a csv file\n",
    "# csv_info_reset.to_csv('/Users/quentin/Desktop/HeadFixedBehavior/Data/all_csv_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Execution stopped",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Break execution at thie cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Execution stopped"
     ]
    }
   ],
   "source": [
    "# Break execution at thie cell\n",
    "raise Exception(\"Execution stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify each unique value in the 'Column' column\n",
    "unique_columns = csv_info_reset['Column'].unique()\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for column in unique_columns:\n",
    "    column_df = csv_info_reset[csv_info_reset['Column'] == column]\n",
    "    unique_file_paths_count = column_df['File_Path'].nunique()\n",
    "\n",
    "    # Breakdown by data type\n",
    "    data_types = column_df[column_df['Info_Type'] == 'unique_types']['Data'].unique()\n",
    "    data_types_breakdown = {}\n",
    "    for data_type in data_types:\n",
    "        # Start by handling the NaN case\n",
    "        if pd.isna(data_type):\n",
    "            data_type_count = column_df[(column_df['Info_Type'] == 'unique_types') & (column_df['Data'].isna())]['File_Path'].nunique()\n",
    "            data_types_breakdown['nan'] = {\n",
    "                'count': data_type_count\n",
    "            }\n",
    "        elif data_type in ['int', 'float', 'double']:\n",
    "            # Extract numeric values\n",
    "            numeric_values = column_df[column_df['Info_Type'] == 'unique_values']['Data']\n",
    "            # Splitting string of numbers into individual numeric values\n",
    "            numeric_values = numeric_values.dropna().apply(lambda x: pd.Series(str(x).split(', '))).stack().reset_index(drop=True)\n",
    "            numeric_values = numeric_values.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "            data_type_count = column_df[(column_df['Info_Type'] == 'unique_types') & (column_df['Data'] == data_type)]['File_Path'].nunique()\n",
    "            # Descriptive summary\n",
    "            descriptive_summary = numeric_values.describe()\n",
    "            # Unique values (up to 10)\n",
    "            unique_values = numeric_values.unique()[:10]\n",
    "            data_types_breakdown[data_type] = {\n",
    "                'count': data_type_count,\n",
    "                'summary': descriptive_summary,\n",
    "                'unique_values': unique_values\n",
    "            }\n",
    "        else:\n",
    "            # For non-numeric types, list all unique values\n",
    "            unique_values = column_df[column_df['Info_Type'] == 'unique_values']['Data'].unique()\n",
    "            data_type_count = column_df[(column_df['Info_Type'] == 'unique_types') & (column_df['Data'] == data_type)]['File_Path'].nunique()\n",
    "            data_types_breakdown[data_type] = {\n",
    "                'count': data_type_count,\n",
    "                'unique_values': unique_values\n",
    "            }\n",
    "\n",
    "    # Add the results to the dictionary\n",
    "    analysis_results[column] = {\n",
    "        'unique_file_paths_count': unique_file_paths_count,\n",
    "        'data_types_breakdown': data_types_breakdown\n",
    "    }\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "analysis_results_df = pd.DataFrame.from_dict(analysis_results, orient='index')\n",
    "analysis_results_df.reset_index(inplace=True)\n",
    "analysis_results_df.rename(columns={'index': 'Column'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Path</th>\n",
       "      <th>Column</th>\n",
       "      <th>Info_Type</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257378</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_7/Trial_Summar...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257670</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_10_13/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257972</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_11_21/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258253</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_19/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258545</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_19/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280857</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_10_10/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281145</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_11_22/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281426</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281718</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282010</th>\n",
       "      <td>QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...</td>\n",
       "      <td>Animal_ID</td>\n",
       "      <td>unique_values</td>\n",
       "      <td>QP0103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                File_Path     Column  \\\n",
       "257378  QP0103/SOUND_CAT_QP0103_2023_12_7/Trial_Summar...  Animal_ID   \n",
       "257670  QP0103/SOUND_CAT_QP0103_2023_10_13/Trial_Summa...  Animal_ID   \n",
       "257972  QP0103/SOUND_CAT_QP0103_2023_11_21/Trial_Summa...  Animal_ID   \n",
       "258253  QP0103/SOUND_CAT_QP0103_2023_12_19/Trial_Summa...  Animal_ID   \n",
       "258545  QP0103/SOUND_CAT_QP0103_2023_12_19/Trial_Summa...  Animal_ID   \n",
       "...                                                   ...        ...   \n",
       "280857  QP0103/SOUND_CAT_QP0103_2023_10_10/Trial_Summa...  Animal_ID   \n",
       "281145  QP0103/SOUND_CAT_QP0103_2023_11_22/Trial_Summa...  Animal_ID   \n",
       "281426  QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...  Animal_ID   \n",
       "281718  QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...  Animal_ID   \n",
       "282010  QP0103/SOUND_CAT_QP0103_2023_12_20/Trial_Summa...  Animal_ID   \n",
       "\n",
       "            Info_Type    Data  \n",
       "257378  unique_values  QP0103  \n",
       "257670  unique_values  QP0103  \n",
       "257972  unique_values  QP0103  \n",
       "258253  unique_values  QP0103  \n",
       "258545  unique_values  QP0103  \n",
       "...               ...     ...  \n",
       "280857  unique_values  QP0103  \n",
       "281145  unique_values  QP0103  \n",
       "281426  unique_values  QP0103  \n",
       "281718  unique_values  QP0103  \n",
       "282010  unique_values  QP0103  \n",
       "\n",
       "[85 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_info_reset[csv_info_reset['Data'] == 'QP0103']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Execution stopped",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Interrup execution at this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Execution stopped"
     ]
    }
   ],
   "source": [
    "# Interrup execution at this cell\n",
    "raise Exception(\"Execution stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QP096:   2%|▏         | 2/89 [00:00<00:22,  3.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QP096: 100%|██████████| 89/89 [00:50<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Volumes/akrami/Quentin/Head_Fixed_Behavior/Data/'\n",
    "protocol = 'SOUND_CAT'\n",
    "data_type = 'Trial_Summary'\n",
    "\n",
    "Animal_List = [\n",
    "    # 'QP093', 'QP094', \n",
    "    'QP096', \n",
    "    # 'QP097', 'QP098', 'QP099', 'QP0100', 'QP0101', 'QP0102', 'QP0103'\n",
    "    ]\n",
    "\n",
    "\n",
    "# Define the path to the cache file\n",
    "cache_path = '/Users/quentin/Desktop/HeadFixedBehavior/Data/'\n",
    "cache_file = os.path.join(cache_path, \"all_data_concat_SC.pkl\")\n",
    "\n",
    "# Define the path to the figures\n",
    "figs_path = '/Users/quentin/Desktop/HeadFixedBehavior/Analysis/Figures/'\n",
    "\n",
    "# Check if the cache file exists\n",
    "if os.path.exists(cache_file):\n",
    "    # If the cache file exists, load the data from the cache\n",
    "    with open(cache_file, \"rb\") as f:       \n",
    "        all_data_concat_SC = pickle.load(f)\n",
    "else:\n",
    "    # If the cache file does not exist, load the data from the remote server\n",
    "    all_data_concat_SC = pd.DataFrame()\n",
    "    for animal in Animal_List:\n",
    "        try:\n",
    "            # Load the data for the current animal and add it to the dictionary\n",
    "            animal_data = get_animal_data(data_path,\n",
    "                                             animal, \n",
    "                                             protocol,\n",
    "                                             data_type)\n",
    "            all_data_concat_SC = pd.concat([all_data_concat_SC, animal_data])\n",
    "        except:\n",
    "            # If there is an error loading the data, skip the current animal\n",
    "            pass\n",
    "\n",
    "    # Save the data to the cache file\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(all_data_concat_SC, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data_concat_SC[all_data_concat_SC.Animal_ID == 'QP093'].reset_index(drop=True).File_ID.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'str': {'count': 944,\n",
       "  'unique_values': array(['QP093', 'TEST', 'QP094', 'QP096', 'QP097', 'QP098', 'QP098, QP0',\n",
       "         'QP099', 'QP0100', 'QP0101', 'QP0103'], dtype=object)}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_col_info(analysis_results_df, 'Animal_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Execution stopped",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Interrup execution at this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Execution stopped"
     ]
    }
   ],
   "source": [
    "# Interrup execution at this cell\n",
    "raise Exception(\"Execution stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Breakdown of steps:\n",
    "# 1. Read all csv files, extremely basic processing (e.g. add Fild_Path tag, add date, etc.) and concatenate together\n",
    "# 2. Remove nan columns\n",
    "# 3. Rename all releveant columns\n",
    "# 4. Remap the values inside those columns\n",
    "# 5. Check and convert datatypes\n",
    "# 6. Check ranges (e.g. reject values of choice that are not 0 or 1)\n",
    "# 7. Deal with missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_values(df, column_info):\n",
    "    '''\n",
    "    Remaps values in a dataframe according to the value_mapping dict in column_info\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to be remapped\n",
    "        column_info (dict): dictionary containing information about the columns\n",
    "    Returns:\n",
    "        df (pd.DataFrame): remapped dataframe\n",
    "    '''\n",
    "    for col, info in column_info.items():\n",
    "        if col in df.columns and info['value_mapping']:\n",
    "            for new_val, old_vals in info['value_mapping'].items():\n",
    "                df[col] = df[col].replace(old_vals, new_val)\n",
    "    return df\n",
    "\n",
    "def check_and_convert_dtypes(df, column_info):\n",
    "    '''\n",
    "    Checks if the dtypes of the columns in the dataframe match the expected dtypes in column_info\n",
    "    and converts them if necessary\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to be checked\n",
    "        column_info (dict): dictionary containing information about the columns\n",
    "    Returns:\n",
    "        df (pd.DataFrame): dataframe with converted dtypes\n",
    "    '''\n",
    "    for col, info in column_info.items():\n",
    "        if col in df.columns:\n",
    "            expected_dtype = info['dtype']\n",
    "            if df[col].dtype != expected_dtype:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(expected_dtype)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not convert column {col} to {expected_dtype}\")\n",
    "    return df\n",
    "\n",
    "def standardize_dataframe(df, column_info):\n",
    "    '''\n",
    "    Standardizes the dataframe by renaming columns, filtering columns, remapping values and converting dtypes\n",
    "    If certain columns are missing, they are added (potentially future arguments to choose whether to add them or not)\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to be standardized\n",
    "        column_info (dict): dictionary containing information about the columns\n",
    "    Returns:\n",
    "        cleaned_df (pd.DataFrame): standardized dataframe\n",
    "    '''\n",
    "    rename_dict = {old_name: new_name for new_name, info in column_info.items() if info['rename'] for old_name in info['rename']}\n",
    "\n",
    "    cleaned_df_list = []\n",
    "\n",
    "    # look through all File_IDs and re-index the 'Trial' column, then re-assign the df\n",
    "    for file_id in df['File_ID'].unique():\n",
    "        file_df = df[df['File_ID'] == file_id]\n",
    "        # drop all columns that are fully NaN\n",
    "        file_df = file_df.dropna(axis=1, how='all')\n",
    "        # rename columns\n",
    "        file_df = file_df.rename(columns=rename_dict)\n",
    "\n",
    "        # remap values\n",
    "        file_df = remap_values(file_df, column_info)\n",
    "\n",
    "        file_df = check_and_convert_dtypes(file_df, column_info)\n",
    "\n",
    "        # re-index the 'Trial' column\n",
    "        file_df['Trial'] = file_df['Trial'].rank(method='dense').astype('Int64')\n",
    "        # # Special handling for the 'choice' column\n",
    "        # if 'choice' in file_df.columns and 2 in file_df['choice'].values:\n",
    "        #     file_df['choice'] = file_df['choice'] - 1\n",
    "\n",
    "        # # if 'trial_outcome' is missing, add it\n",
    "        # if 'Trial_Outcome' not in file_df.columns:\n",
    "        #     # check if 'correct' and 'no_response' are present\n",
    "        #     if 'correct' in file_df.columns and 'no_response' in file_df.columns:\n",
    "\n",
    "        #         file_df.loc[(file_df['correct'] == 1) & ~file_df['no_response'], 'Trial_Outcome'] = 'Correct'\n",
    "        #         file_df.loc[(file_df['correct'] == 0) & ~file_df['no_response'], 'Trial_Outcome'] = 'Incorrect'\n",
    "        #         file_df.loc[file_df['no_response'], 'Trial_Outcome'] = 'No_Response'\n",
    "        #         file_df['Trial_Outcome'].fillna('Unknown', inplace=True)\n",
    "\n",
    "        # if 'Stim_Relative' not in file_df.columns and 'WN_Amp' in file_df.columns:\n",
    "        #     file_df['Stim_Relative'] = file_df['WN_Amp'].apply(lambda x: convert_value(x, \n",
    "        #                                                                                original_min=50, original_max=82, \n",
    "        #                                                                                new_min=-1, new_max=1))\n",
    "        # if 'Nb_Of_Stim' not in file_df.columns and file_df['Stage'].iloc[0] != 'Full_Task_Cont':\n",
    "        #     try:\n",
    "        #         file_df['Nb_Of_Stim'] = len(file_df.Stim_Relative.unique()) \n",
    "        #     except AttributeError:\n",
    "        #         file_df['Nb_Of_Stim'] = np.nan\n",
    " \n",
    "        # only keep columns that are in the column_info dict\n",
    "        file_df = file_df[[col for col in file_df.columns if col in column_info]]\n",
    "\n",
    "        cleaned_df_list.append(file_df)\n",
    "\n",
    "        \n",
    "\n",
    "    cleaned_df = pd.concat(cleaned_df_list, ignore_index=True)\n",
    "    return cleaned_df\n",
    "\n",
    "def convert_value(original_value, original_min=50, original_max=82, new_min=-1, new_max=1):\n",
    "    # Translate original_value to the new range\n",
    "    return (original_value - original_min) * (new_max - new_min) / (original_max - original_min) + new_min\n",
    "\n",
    "def get_col_info(df, column):\n",
    "    '''\n",
    "    Returns the column information for a given column\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing the column\n",
    "        column (str): column name\n",
    "    Returns:\n",
    "        result (dict): dictionary containing the column information\n",
    "    '''\n",
    "    result = df[df['Column'] == column].data_types_breakdown.iloc[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_info = {\n",
    "    'Date': {'dtype': str, \n",
    "             'rename': None, \n",
    "             'value_mapping': None},\n",
    "    'Participant_ID': {'dtype': str, \n",
    "                       'rename': ['Animal_ID'], \n",
    "                       'value_mapping': None},\n",
    "    'Protocol': {'dtype': str,\n",
    "                 'rename': None,\n",
    "                 'value_mapping': None},\n",
    "    'Stage': {'dtype': str,\n",
    "                'rename': None,\n",
    "                'value_mapping': {'Habituation': [1, 'Habituation'],\n",
    "                                  'Lick_To_Release': [2, 'Lick_To_Release'],\n",
    "                                  'Three_And_Three': [3, 'Three_And_Three'],\n",
    "                                  'Full_Task_Disc': [4, 'Full_Task_Disc'],\n",
    "                                  'Full_Task_Cont': [5, 'Full_Task_Cont']}},\n",
    "    'Trial': {'dtype': 'Int64',\n",
    "                'rename': ['TrialNumber', 'Trial_Number'],\n",
    "                'value_mapping': None},\n",
    "    'choice': {'dtype': 'Int64', \n",
    "               'rename': ['FirstLick'], \n",
    "               'value_mapping': {0: 'Left', 1: 'Right'}},\n",
    "    'Stim_Relative': {'dtype': float,\n",
    "                        'rename': None,\n",
    "                        'value_mapping': None},\n",
    "    'correct': {'dtype': 'Int64',\n",
    "                'rename': ['Correct', 'correct'],\n",
    "                'value_mapping': {1: [True, '1', 'True'],\n",
    "                                  0: [False, '0', 'False']}},\n",
    "    'Trial_Outcome': {'dtype': str, \n",
    "                      'rename': ['TrialOutcome'], \n",
    "                      'value_mapping': {'Correct': ['Correct', 'correct', 1], \n",
    "                                        'Incorrect': ['Incorrect', 'incorrect', 0], \n",
    "                                        'No_Response': ['No Response', 'no_response', -1, 'Abort']}},\n",
    "    'no_response': {'dtype': bool,\n",
    "                    'rename': ['AbortTrial', 'Abort_Trial'],\n",
    "                    'value_mapping': None},\n",
    "    'Response_Latency': {'dtype': float,\n",
    "                        'rename': None,\n",
    "                        'value_mapping': None},\n",
    "    'Nb_Of_Stim': {'dtype': 'Int32',\n",
    "                    'rename': None,\n",
    "                    'value_mapping': None},\n",
    "    'Stim_Type': {'dtype': str,\n",
    "                  'rename': None,\n",
    "                  'value_mapping': None},\n",
    "    'Anti_Bias': {'dtype': bool,\n",
    "                  'rename': ['AntiBias', 'Anti_Bias'],\n",
    "                  'value_mapping': None},\n",
    "    'Sound_Contingency': {'dtype': str,\n",
    "                          'rename': None,\n",
    "                          'value_mapping': None},\n",
    "    'P_Right': {'dtype': float,\n",
    "                'rename': None,\n",
    "                'value_mapping': None},\n",
    "    'Distribution': {'dtype': str,\n",
    "                     'rename': None,\n",
    "                     'value_mapping': None},\n",
    "    'Trial_End_Time': {'dtype': float,\n",
    "                      'rename': ['Time', 'Trial_End_Time'],\n",
    "                      'value_mapping': None},\n",
    "    'File_ID': {'dtype': str,   \n",
    "                   'rename': None,\n",
    "                   'value_mapping': None},\n",
    "    'WN_Amp': {'dtype': float,\n",
    "                'rename': None,\n",
    "                'value_mapping': None},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_concat_SC.Trial_End_Time.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_df = standardize_dataframe(all_data_concat_SC, column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrup execution at this cell\n",
    "raise Exception(\"Execution stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check caolumn outputs align with expected outputs\n",
    "\n",
    "# # where choice is not 0 or 1, set it to NaN\n",
    "# pa_df.loc[~pa_df.choice.isin([0, 1]), 'choice'] = np.nan\n",
    "# # where correct is not 0 or 1, set it to NaN\n",
    "# pa_df.loc[~pa_df.correct.isin([0, 1]), 'correct'] = np.nan\n",
    "# # where Trial_Outcome is not 'Correct', 'Incorrect' or 'No_Response', set it to NaN\n",
    "# pa_df.loc[~pa_df.Trial_Outcome.isin(['Correct', 'Incorrect', 'No_Response']), 'Trial_Outcome'] = np.nan\n",
    "# # where Air_Puff_Side is not 0 or 1, set it to NaN\n",
    "# pa_df.loc[~pa_df.Air_Puff_Side.isin([0, 1]), 'Air_Puff_Side'] = np.nan\n",
    "# # where Air_Puff_Contingency is not 'Pro' or 'Anti', set it to NaN\n",
    "# pa_df.loc[~pa_df.Air_Puff_Contingency.isin(['Pro', 'Anti']), 'Air_Puff_Contingency'] = np.nan\n",
    "# # where Stage is not 'Habituation', 'Lick_To_Release', 'Three_And_Three', 'Full_Task_Disc' or 'Full_Task_Cont', set it to NaN\n",
    "# pa_df.loc[~pa_df.Stage.isin(['Habituation', 'Lick_To_Release', 'Three_And_Three', 'Full_Task_Disc', 'Full_Task_Cont']), 'Stage'] = np.nan\n",
    "# # where Protocol is not 'PRO_ANTI', set it to NaN\n",
    "# pa_df.loc[~pa_df.Protocol.isin(['PRO_ANTI']), 'Protocol'] = np.nan\n",
    "# # where Sound_Contingency is not 'Low_Left_High_Right' or 'Low_Right_High_Left', set it to NaN\n",
    "# pa_df.loc[~pa_df.Sound_Contingency.isin(['Low_Left_High_Right', 'Low_Right_High_Left']), 'Sound_Contingency'] = np.nan\n",
    "# # where Stim_Type is not 'WN' or 'PT', set it to NaN\n",
    "# pa_df.loc[~pa_df.Stim_Type.isin(['WN', 'PT']), 'Stim_Type'] = np.nan\n",
    "# # if Distribution is not 'Uniform', 'Asym_Left' or 'Asym_Right', set it to NaN\n",
    "# pa_df.loc[~pa_df.Distribution.isin(['Uniform', 'Asym_Left', 'Asym_Right']), 'Distribution'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dealing with missing values\n",
    "# cleaned_df_list = []\n",
    "\n",
    "# for file_id in pa_df.File_ID.unique():\n",
    "#     file_df = pa_df[pa_df['File_ID'] == file_id].reset_index(drop=True)\n",
    "\n",
    "#     if 'Trial_Outcome' not in file_df.columns:\n",
    "#         # check if 'correct' and 'no_response' are present\n",
    "#         if 'correct' in file_df.columns and 'no_response' in file_df.columns:\n",
    "#             file_df.loc[(file_df['correct'] == 1) & ~file_df['no_response'], 'Trial_Outcome'] = 'Correct'\n",
    "#             file_df.loc[(file_df['correct'] == 0) & ~file_df['no_response'], 'Trial_Outcome'] = 'Incorrect'\n",
    "#             file_df.loc[file_df['no_response'], 'Trial_Outcome'] = 'No_Response'\n",
    "#             file_df['Trial_Outcome'].fillna('Unknown', inplace=True)\n",
    "            \n",
    "#     if 'Nb_Of_Stim' not in file_df.columns and file_df['Stage'].iloc[0] != 'Full_Task_Cont':\n",
    "#         try:\n",
    "#             file_df['Nb_Of_Stim'] = len(file_df.Stim_Relative.unique()) \n",
    "#         except AttributeError:\n",
    "#             file_df['Nb_Of_Stim'] = np.nan\n",
    "#     if 'Stim_Relative' not in file_df.columns and 'WN_Amp' in file_df.columns:\n",
    "#         file_df['Stim_Relative'] = file_df['WN_Amp'].apply(lambda x: convert_value(x, \n",
    "#                                                                                    original_min=50, original_max=82, \n",
    "#                                                                                    new_min=-1, new_max=1))\n",
    "\n",
    "#     cleaned_df_list.append(file_df)\n",
    "\n",
    "# pa_df = pd.concat(cleaned_df_list, ignore_index=True)\n",
    "# # we know the protocol is PRO_ANTI, so we can replace the NaNs in the Protocol column with 'PRO_ANTI'\n",
    "# pa_df.Protocol.fillna('PRO_ANTI', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: account for extremely large negaticve values in Response_Latency (because of how nans are written in Bonsai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Execution stopped",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Interrup execution at this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Execution stopped"
     ]
    }
   ],
   "source": [
    "# Interrup execution at this cell\n",
    "raise Exception(\"Execution stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound_cat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
